import os
# os.environ["CUDA_VISIBLE_DEVICES"] = "3"
import json
import math
import argparse
from pathlib import Path
import sys
import re
import concurrent.futures
from threading import Lock

# Set environment variables
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["OMP_NUM_THREADS"] = "4"
os.environ["MKL_NUM_THREADS"] = "4"

from tqdm import tqdm

# Import utility classes and functions
from dl_utils import BaseRunner, exp_safe

# All backend-specific packages are imported on-demand within their respective classes
# No longer importing transformers, openai, etc. at the top level


class OpenAIResult:
    """Unified OpenAI API result class"""
    
    def __init__(self, text, choice, logprobs_supported=True):
        self.text = text
        self.choice = choice
        self.logprobs_supported = logprobs_supported
        
        # Build outputs attribute for vLLM format compatibility
        self.outputs = [self]
        
        # Process logprobs information
        self.logprobs_data = None
        if logprobs_supported and choice and hasattr(choice, 'logprobs') and choice.logprobs:
            self.logprobs_data = self._process_logprobs(choice.logprobs)
    
    def _process_logprobs(self, logprobs):
        """Process OpenAI logprobs data and convert to unified format"""
        processed_logprobs = []
        
        if hasattr(logprobs, 'content') and logprobs.content:
            for token_logprob in logprobs.content:
                token_data = {
                    'token': token_logprob.token,
                    'logprob': token_logprob.logprob,
                    'top_logprobs': []
                }
                
                if hasattr(token_logprob, 'top_logprobs') and token_logprob.top_logprobs:
                    for top_choice in token_logprob.top_logprobs:
                        token_data['top_logprobs'].append({
                            'token': top_choice.token,
                            'logprob': top_choice.logprob
                        })
                
                processed_logprobs.append(token_data)
        
        return processed_logprobs


class VLLMRunner(BaseRunner):
    """vLLM inference engine"""
    
    def __init__(self, model_name, temperature=0.0):
        super().__init__(model_name, temperature)
        
        # Import vLLM and related packages on-demand, only when actually used
        try:
            from vllm import LLM, SamplingParams
            from transformers import AutoTokenizer
        except ImportError as e:
            raise ImportError(
                f"vLLM and transformers are required for local model inference. "
                f"Please install them with: pip install vllm transformers\n"
                f"Original error: {e}"
            )
        
        self.top_p = 0.7
        self.top_k = 50
        # top_logprobs is now unified in BaseRunner

        print(f"INFO: Initializing vLLM model: {model_name}")

        # Initialize tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
        self.client = LLM(
            model=model_name,
            tensor_parallel_size=1,
            max_model_len=131072,
            rope_scaling={
                "type": "yarn",
                "rope_type": "yarn", 
                "factor": 4.0,
                "original_max_position_embeddings": 32768,
            },
            rope_theta=1_000_000.0,
            gpu_memory_utilization=0.89,
            swap_space=4,
            trust_remote_code=True,
        )
        
        self.sampling_params = SamplingParams(
            max_tokens=self.max_tokens,
            top_k=self.top_k,
            top_p=self.top_p,
            temperature=self.temperature,
            logprobs=self.top_logprobs,
            include_stop_str_in_output=False
        )
        
        print(f"SUCCESS: vLLM Model loaded successfully!")

    def generate(self, prompts):
        """Generate text using vLLM"""
        return self.client.generate(prompts, self.sampling_params)


class OpenAIRunner(BaseRunner):
    """OpenAI API inference engine (with parallelization support)"""
    
    def __init__(self, model_name, temperature=0.0, api_key=None, base_url=None, max_workers=4):
        super().__init__(model_name, temperature)

        # Import OpenAI package on-demand, only when actually used
        try:
            import openai
        except ImportError as e:
            raise ImportError(
                f"openai package is required for OpenAI API inference. "
                f"Please install it with: pip install openai\n"
                f"Original error: {e}"
            )

        print(f"ðŸŒ Initializing OpenAI API with model: {model_name}")
        self.temperature = temperature
        self.max_workers = max_workers

        # Use parameters first, then environment variables
        final_api_key = api_key or os.getenv('OPENAI_API_KEY')
        final_base_url = base_url or os.getenv('OPENAI_BASE_URL')
        
        if not final_api_key:
            raise ValueError("API key is required for OpenAI. Please provide --api_key or set OPENAI_API_KEY environment variable")
        if not final_base_url:
            raise ValueError("Base URL is required for OpenAI. Please provide --base_url or set OPENAI_BASE_URL environment variable")
        
        self.client = openai.OpenAI(api_key=final_api_key, base_url=final_base_url)
        self.logprobs_supported = True
        
        # Thread-safe lock
        self.lock = Lock()
        
        print(f"SUCCESS: OpenAI API initialized successfully!")
        print(f"   API Key: {final_api_key[:8]}...")
        print(f"   Base URL: {final_base_url}")
        print(f"   Max Workers: {max_workers}")

    def _generate_single(self, prompt):
        """Generate response for a single prompt"""
        try:
            # Build message format
            messages = [{"role": "user", "content": prompt}]
            
            params = {
                "model": self.model_name,
                "messages": messages,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            }
            
            # If logprobs is supported, add related parameters
            if self.logprobs_supported:
                params.update({
                    "logprobs": True,
                    "top_logprobs": self.top_logprobs
                })
            
            response = self.client.chat.completions.create(**params)
            
            if response and hasattr(response, 'choices') and len(response.choices) > 0:
                choice = response.choices[0]
                generated_text = choice.message.content
                
                # Build unified result object
                result = OpenAIResult(generated_text, choice, self.logprobs_supported)
                
                # Thread-safe log output
                with self.lock:
                    print(f"Tokens => Prompt: {response.usage.prompt_tokens}, "
                          f"Completion: {response.usage.completion_tokens}, "
                          f"Total: {response.usage.total_tokens}")
                
                return result
            else:
                with self.lock:
                    print(f"WARNING:  Empty response from OpenAI API")
                return OpenAIResult("", None, False)
                
        except Exception as e:
            with self.lock:
                print(f"WARNING:  Error calling OpenAI API: {e}")
            return OpenAIResult("", None, False)

    def generate(self, prompts):
        """Generate text using OpenAI API in parallel"""
        results = [None] * len(prompts)  # Pre-allocate result array to maintain order
        
        # Use ThreadPoolExecutor for parallelization
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # Submit all tasks and remember their indices
            future_to_index = {
                executor.submit(self._generate_single, prompt): idx 
                for idx, prompt in enumerate(prompts)
            }
            
            # Collect results with tqdm progress bar
            with tqdm(total=len(prompts), desc="OpenAI API parallel inference") as pbar:
                for future in concurrent.futures.as_completed(future_to_index):
                    idx = future_to_index[future]
                    try:
                        result = future.result()
                        results[idx] = result
                    except Exception as exc:
                        with self.lock:
                            print(f"WARNING:  Prompt {idx} generated an exception: {exc}")
                        results[idx] = OpenAIResult("", None, False)
                    pbar.update(1)
        
        return results


def create_runner(model_name, temperature=0.0, api_key=None, base_url=None, max_workers=4, logprobs_supported=True):
    """Create corresponding inference engine based on model_name"""
    
    if '::' in model_name:
        # Parse format: backend::actual_model_name
        backend, actual_model = model_name.split('::', 1)
        backend = backend.lower()
        
        if backend == 'openai':
            runner = OpenAIRunner(actual_model, temperature, api_key, base_url, max_workers)
            runner.logprobs_supported = logprobs_supported
            return runner
        else:
            raise ValueError(f"Unsupported backend: {backend}. Supported: openai")
    else:
        # Default to vLLM (local model), no parallelization support (GPU already parallel)
        runner = VLLMRunner(model_name, temperature)
        runner.logprobs_supported = logprobs_supported
        return runner


def extract_model_prefix(model_name):
    """
    Extract model name prefix for file naming
    
    Rules:
    1. If in openai:: format, take the part after ::
    2. If contains /, only take the last part (e.g., minzl/toy_3550 -> toy_3550)
    3. Otherwise use directly
    
    Args:
        model_name: Full model name
        
    Returns:
        str: Model prefix for file naming
    """
    if '::' in model_name:
        # Handle backend::model format
        backend, actual_model = model_name.split('::', 1)
        if backend.lower() == 'openai':
            # For openai::model, use the model name after ::
            model_name = actual_model
        else:
            # For other backends, use the full format
            model_name = model_name.replace('::', '_')
    
    # Handle HuggingFace format paths (e.g., minzl/toy_3550)
    if '/' in model_name:
        model_name = model_name.split('/')[-1]
    
    # Replace other possible special characters with underscores
    model_name = model_name.replace('-', '_').replace('.', '_')
    
    return model_name


def construct_file_paths(input_dir, output_dir, dataset_name, split_seed, row_shuffle_seed, 
                        train_chunk_size, test_chunk_size, model_name):
    """Construct file paths based on parameters"""
    
    # Extract model prefix
    model_prefix = extract_model_prefix(model_name)
    
    # Unified subdirectory format: dataset_Sseed{split_seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}
    subdir = f"{dataset_name}_Sseed{split_seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}"
    
    # Input file path
    input_filename = f"{dataset_name}_Rseed{row_shuffle_seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}.jsonl"
    input_jsonl = os.path.join(input_dir, dataset_name, subdir, input_filename)
    
    # Output file path - use model prefix at the beginning, remove _pred suffix
    output_filename = f"{model_prefix}@@{dataset_name}_Rseed{row_shuffle_seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}.jsonl"
    output_jsonl = os.path.join(output_dir, dataset_name, subdir, output_filename)
    
    return input_jsonl, output_jsonl


def determine_input_output_paths(input_dir, output_dir, dataset_name=None, 
                                split_seed=42, row_shuffle_seed=123, train_chunk_size=600, 
                                test_chunk_size=7, model_name=None):
    """
    Intelligently determine input/output paths using new simplified logic
    
    Args:
        input_dir: Input path (could be .jsonl file or directory)
        output_dir: Output path (could be .jsonl file or directory)
        dataset_name: Dataset name (required when input/output is directory)
        model_name: Model name (required when output is directory)
        Other parameters: Parameters for building complex file paths
        
    Returns:
        tuple: (input_file_path, output_file_path, is_single_mode)
    """
    
    # åˆ¤æ–­è¾“å…¥è·¯å¾„ç±»åž‹
    if input_dir.endswith('.jsonl'):
        # è¾“å…¥æ˜¯JSONLæ–‡ä»¶ï¼Œç›´æŽ¥ä½¿ç”¨
        input_file = input_dir
        is_single_mode = True
    else:
        # è¾“å…¥æ˜¯ç›®å½•ï¼Œéœ€è¦æž„å»ºå¤æ‚è·¯å¾„
        if not dataset_name:
            raise ValueError("When input_dir is a directory, dataset_name is required")
        
        # ä½¿ç”¨å¤æ‚çš„è·¯å¾„æž„å»ºé€»è¾‘
        input_file, _ = construct_file_paths(
            input_dir, "", dataset_name, split_seed, row_shuffle_seed,
            train_chunk_size, test_chunk_size, model_name or "dummy"
        )
        is_single_mode = False
    
    # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"Input file not found: {input_file}")
    
    # åˆ¤æ–­è¾“å‡ºè·¯å¾„ç±»åž‹
    if output_dir.endswith('.jsonl'):
        # è¾“å‡ºæ˜¯JSONLæ–‡ä»¶ï¼Œç›´æŽ¥ä½¿ç”¨å¹¶åˆ›å»ºç›®å½•
        output_file = output_dir
        output_dirname = os.path.dirname(output_file)
        if output_dirname:  # åªæœ‰å½“ç›®å½•åä¸ä¸ºç©ºæ—¶æ‰åˆ›å»º
            os.makedirs(output_dirname, exist_ok=True)
    else:
        # è¾“å‡ºæ˜¯ç›®å½•ï¼Œéœ€è¦æž„å»ºå¤æ‚è·¯å¾„
        if not dataset_name:
            raise ValueError("When output_dir is a directory, dataset_name is required")
        if not model_name:
            raise ValueError("When output_dir is a directory, model_name is required")
        
        # ä½¿ç”¨å¤æ‚çš„è·¯å¾„æž„å»ºé€»è¾‘
        _, output_file = construct_file_paths(
            "", output_dir, dataset_name, split_seed, row_shuffle_seed,
            train_chunk_size, test_chunk_size, model_name
        )
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    return input_file, output_file, is_single_mode


def main():
    parser = argparse.ArgumentParser(description='æ‰¹é‡ä¼˜åŒ–ç‰ˆ vLLM æŽ¨ç†è„šæœ¬ï¼ˆæ”¯æŒAPIå¹¶è¡ŒåŒ–ï¼‰')
    
    # ç»Ÿä¸€çš„è¾“å…¥è¾“å‡ºå‚æ•°
    parser.add_argument("--input_dir", 
                        help="è¾“å…¥è·¯å¾„ï¼šå¯ä»¥æ˜¯ç›®å½•ï¼ˆéœ€è¦é…åˆdataset_namesç­‰å‚æ•°è‡ªåŠ¨æž„å»ºæ–‡ä»¶è·¯å¾„ï¼‰æˆ–ç›´æŽ¥çš„.jsonlæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", 
                        help="è¾“å‡ºè·¯å¾„ï¼šå¯ä»¥æ˜¯ç›®å½•ï¼ˆè‡ªåŠ¨æž„å»ºæ–‡ä»¶è·¯å¾„ï¼‰æˆ–ç›´æŽ¥çš„.jsonlæ–‡ä»¶è·¯å¾„ï¼ˆè‡ªåŠ¨åˆ›å»ºå±‚çº§ç›®å½•ï¼‰")
    
    # æ‰¹é‡å¤„ç†å‚æ•°
    parser.add_argument("--dataset_names", 
                        help="æ•°æ®é›†åç§°åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œå¦‚ï¼š'bank heloc rl'")
    parser.add_argument("--row_shuffle_seeds", 
                        help="è¡Œæ‰“ä¹±ç§å­åˆ—è¡¨ï¼Œç”¨ç©ºæ ¼åˆ†éš”ï¼Œå¦‚ï¼š'40 41 42'")

    # è·¯å¾„æž„å»ºå‚æ•°ï¼ˆå½“input_dir/output_diræ˜¯ç›®å½•æ—¶ç”¨äºŽæž„å»ºå¤æ‚æ–‡ä»¶è·¯å¾„ï¼‰
    parser.add_argument("--split_seed", type=int, default=42,
                       help="åˆ†å‰²ç§å­ï¼Œç”¨äºŽæ•°æ®é›†åˆ†å‰²å’Œç¡®å®šæ€§é‡‡æ ·ï¼ˆé»˜è®¤ï¼š42ï¼‰")
    parser.add_argument("--train_chunk_size", type=int, default=600,
                        help="è®­ç»ƒå—å¤§å°ï¼Œæ¯ä¸ªpromptä¸­åŒ…å«çš„few-shotç¤ºä¾‹æ•°é‡ï¼ˆé»˜è®¤ï¼š600ï¼‰")
    parser.add_argument("--test_chunk_size", type=int, default=7,
                        help="æµ‹è¯•å—å¤§å°ï¼Œèšåˆåˆ°å•ä¸ªpromptä¸­è¿›è¡ŒLLMæŽ¨ç†çš„é¡¹ç›®æ•°é‡ï¼ˆé»˜è®¤ï¼š7ï¼‰")

    parser.add_argument('--labels', type=str, help='æ ‡ç­¾åˆ—è¡¨ï¼Œç”¨é€—å·åˆ†éš”ï¼Œå¦‚ï¼š"0,1,2,3,4"')

   # å¿…éœ€å‚æ•°
    parser.add_argument('--model_name', type=str, required=True, 
                        help='æ¨¡åž‹åç§°æˆ–è·¯å¾„ã€‚æ”¯æŒæ ¼å¼ï¼š\n'
                             '  - æœ¬åœ°æ¨¡åž‹è·¯å¾„ (ä½¿ç”¨vLLM): /path/to/model\n'
                             '  - OpenAI API: openai::gpt-4o')
    
    # OpenAI API å‚æ•°ï¼ˆä»…åœ¨ä½¿ç”¨OpenAIæ—¶éœ€è¦ï¼‰
    parser.add_argument('--api_key', type=str, help='OpenAI APIå¯†é’¥ (ä½¿ç”¨openai::æ¨¡åž‹æ—¶å¿…éœ€)')
    parser.add_argument('--base_url', type=str, help='OpenAI APIåŸºç¡€URL (ä½¿ç”¨openai::æ¨¡åž‹æ—¶å¿…éœ€)')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('--temperature', type=float, default=0.0, help='é‡‡æ ·æ¸©åº¦')
    parser.add_argument('--max_samples', type=int, default=None, help='æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--device_id', type=str, default="0", help='æŒ‡å®šä½¿ç”¨çš„GPUè®¾å¤‡ID (é»˜è®¤: "0")')
    parser.add_argument('--force_overwrite', action='store_true', default=False,
                        help='å¦‚æžœè®¾ç½®ï¼Œå°†ç›´æŽ¥åˆ é™¤å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶è€Œä¸è¯¢é—®')
    
    # å¹¶è¡ŒåŒ–å‚æ•°
    parser.add_argument('--max_workers', type=int, default=4,
                        help='APIå¹¶è¡ŒæŽ¨ç†çš„æœ€å¤§å·¥ä½œçº¿ç¨‹æ•°ï¼ˆä»…å¯¹openai::æœ‰æ•ˆï¼Œé»˜è®¤ï¼š4ï¼‰')
    parser.add_argument("--logprobs-supported", dest="logprobs_supported", 
                        action=argparse.BooleanOptionalAction, default=True,
                        help="Enable/disable logprobs support")
    
    args = parser.parse_args()
    
    # è®¾ç½® GPU è®¾å¤‡
    if args.device_id:
        os.environ["CUDA_VISIBLE_DEVICES"] = args.device_id
        print(f"TARGET: è®¾ç½® GPU è®¾å¤‡: {args.device_id}")
    
    if not args.model_name:
        print("ERROR: Error: --model_name is required")
        sys.exit(1)
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not args.input_dir or not args.output_dir:
        print("ERROR: Error: Both --input_dir and --output_dir are required")
        print("   --input_dir: å¯ä»¥æ˜¯ç›®å½•æˆ– .jsonl æ–‡ä»¶è·¯å¾„")
        print("   --output_dir: å¯ä»¥æ˜¯ç›®å½•æˆ– .jsonl æ–‡ä»¶è·¯å¾„")
        sys.exit(1)
    
    # è§£æžæ‰¹é‡å‚æ•°
    dataset_names = args.dataset_names.split() if args.dataset_names else []
    row_shuffle_seeds = [int(x) for x in args.row_shuffle_seeds.split()] if args.row_shuffle_seeds else []
    
    if not dataset_names or not row_shuffle_seeds:
        print("ERROR: Error: Both --dataset_names and --row_shuffle_seeds are required for batch processing")
        print("   --dataset_names: 'bank heloc rl'")
        print("   --row_shuffle_seeds: '40 41 42'")
        sys.exit(1)
    
    # æ£€æŸ¥OpenAIç›¸å…³å‚æ•°
    if '::' in args.model_name:
        backend, _ = args.model_name.split('::', 1)
        backend = backend.lower()
        
        if backend == 'openai':
            # æ£€æŸ¥APIå¯†é’¥å’Œbase_url
            api_key = args.api_key or os.getenv('OPENAI_API_KEY')
            base_url = args.base_url or os.getenv('OPENAI_BASE_URL')
            
            if not api_key:
                print("ERROR: Error: OpenAI API key is required when using openai:: backend")
                print("   Please provide --api_key or set OPENAI_API_KEY environment variable")
                sys.exit(1)
            
            if not base_url:
                print("ERROR: Error: OpenAI base URL is required when using openai:: backend")
                print("   Please provide --base_url or set OPENAI_BASE_URL environment variable")
                sys.exit(1)
    
    # åˆå§‹åŒ–æŽ¨ç†å™¨ï¼ˆåªåˆå§‹åŒ–ä¸€æ¬¡ï¼ï¼‰
    print(f"STARTING: Initializing model once for all datasets and seeds...")
    
    # æ£€æŸ¥å¹¶è¡ŒåŒ–æ˜¯å¦é€‚ç”¨
    if '::' in args.model_name:
        backend, _ = args.model_name.split('::', 1)
        backend = backend.lower()
        if backend == 'openai':
            print(f"ðŸ”„ APIå¹¶è¡ŒåŒ–å¯ç”¨ - Max workers: {args.max_workers}")
    
    runner = create_runner(args.model_name, args.temperature, args.api_key, args.base_url, args.max_workers, args.logprobs_supported)
    
    # è§£æžç”¨æˆ·æä¾›çš„æ ‡ç­¾
    user_labels = None
    if args.labels:
        # è§£æžé€—å·åˆ†éš”çš„æ ‡ç­¾å­—ç¬¦ä¸²
        label_list = [label.strip() for label in args.labels.split(',')]
        # æŽ’åºæ ‡ç­¾ï¼šæ•°å­—ä¼˜å…ˆï¼Œç„¶åŽå­—æ¯
        user_labels = sorted(label_list, key=lambda x: (not x.isdigit(), int(x) if x.isdigit() else float('inf'), x))
        print(f"INFO: Using user-provided labels: {user_labels}")
    else:
        print(f"INFO: No labels provided, will auto-infer from file or label_transform_info.json")
    
    # è®¡ç®—æ€»ä»»åŠ¡æ•°
    total_tasks = len(dataset_names) * len(row_shuffle_seeds)
    current_task = 0
    successful_tasks = 0
    failed_tasks = 0
    
    print(f"INFO: Batch processing summary:")
    print(f"   Datasets: {dataset_names}")
    print(f"   Row shuffle seeds: {row_shuffle_seeds}")
    print(f"   Total tasks: {total_tasks}")
    print(f"   Model will be loaded only once! TARGET:")
    print("")
    
    try:
        # åŒé‡å¾ªçŽ¯å¤„ç†æ‰€æœ‰ç»„åˆ
        for dataset_name in dataset_names:
            for row_shuffle_seed in row_shuffle_seeds:
                current_task += 1
                
                print(f"+-----------------------------------------------------------------------------+")
                print(f"| Task {current_task:2d}/{total_tasks:2d} | Dataset: {dataset_name:10s} | Seed: {row_shuffle_seed:4d} |")
                print(f"+-----------------------------------------------------------------------------+")
                
                try:
                    # æ™ºèƒ½åˆ¤æ–­è¾“å…¥è¾“å‡ºè·¯å¾„
                    input_file, output_file, is_single_mode = determine_input_output_paths(
                        args.input_dir, args.output_dir, dataset_name,
                        args.split_seed, row_shuffle_seed, args.train_chunk_size,
                        args.test_chunk_size, args.model_name
                    )
                    
                    print(f"INPUT: File paths:")
                    print(f"   Input:  {input_file}")
                    print(f"   Output: {output_file}")
                    
                    # å¤„ç†æ–‡ä»¶
                    success = runner.process_file(
                        input_file, 
                        output_file, 
                        max_samples=args.max_samples,
                        user_labels=user_labels,
                        force_overwrite=args.force_overwrite
                    )
                    
                    if success:
                        print(f"SUCCESS: Successfully processed: {dataset_name} (seed: {row_shuffle_seed})")
                        successful_tasks += 1
                    else:
                        print(f"ERROR: Failed to process: {dataset_name} (seed: {row_shuffle_seed})")
                        failed_tasks += 1
                        
                except (FileNotFoundError, ValueError) as e:
                    print(f"ERROR: Error processing {dataset_name} (seed: {row_shuffle_seed}): {e}")
                    failed_tasks += 1
                
                print("")
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        print("COMPLETED: Batch processing completed!")
        print(f"INFO: Final statistics:")
        print(f"   Total tasks: {total_tasks}")
        print(f"   Successful: {successful_tasks}")
        print(f"   Failed: {failed_tasks}")
        print(f"   Success rate: {successful_tasks/total_tasks*100:.1f}%")
        
        if failed_tasks > 0:
            sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nWARNING: Processing interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"ERROR: Unexpected error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
