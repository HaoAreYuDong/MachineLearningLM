"""
å¤šæ•°æŠ•ç¥¨è¯„ä¼°å™¨å·¥å…·å‡½æ•°
"""

import os
import json
import re
from collections import defaultdict
import numpy as np


def parse_and_unify_json(response_str, prob_mapping=None, default_label=0):
    """
    è§£æå¹¶ç»Ÿä¸€JSONå“åº”æ ¼å¼
    
    Args:
        response_str: åŸå§‹å“åº”å­—ç¬¦ä¸²
        prob_mapping: æ¦‚ç‡æ˜ å°„å­—å…¸ {item_id: {"0": prob0, "1": prob1, ...}}
        default_label: å½“æ— æ³•æ¨æ–­æ ‡ç­¾æ—¶ä½¿ç”¨çš„é»˜è®¤æ ‡ç­¾
        
    Returns:
        list: ç»Ÿä¸€æ ¼å¼çš„é¢„æµ‹ç»“æœåˆ—è¡¨
    """
    response_str = response_str.replace("test_id", "id")
    cleaned = re.sub(r'\r\n|\r', '\n', response_str.strip())
    cleaned = re.sub(r'[\x00-\x1F]+', ' ', cleaned)
    
    # Fix malformed JSON arrays
    cleaned = re.sub(r'\[\s*"([^"]+)"\s*:\s*([^,\]]+)\s*,\s*"([^"]+)"\s*:\s*([^,\]]+)\s*\]', 
                    r'{"\\1": \\2, "\\3": \\4}', cleaned)
    
    try:
        parsed_json = json.loads(cleaned.replace("'", '"'))
    except json.JSONDecodeError:
        json_candidates = []
        stack = []
        start_idx = -1
        
        for i, char in enumerate(cleaned):
            if char in '{[':
                if not stack:
                    start_idx = i
                stack.append(char)
            elif char in '}]':
                if stack:
                    stack.pop()
                    if not stack and start_idx != -1:
                        json_candidates.append(cleaned[start_idx:i + 1])
                        start_idx = -1
        
        parsed_json = None
        for candidate in sorted(json_candidates, key=len, reverse=True):
            try:
                candidate = re.sub(r'\[\s*"([^"]+)"\s*:\s*([^,\]]+)\s*,\s*"([^"]+)"\s*:\s*([^,\]]+)\s*\]', 
                                 r'{"\\1": \\2, "\\3": \\4}', candidate)
                candidate = re.sub(r'(?m)^\s*(//|#).*?\n', '', candidate)
                candidate = re.sub(r'/\*.*?\*/', '', candidate, flags=re.DOTALL)
                candidate = re.sub(r',\s*]', ']', candidate)
                candidate = re.sub(r',\s*}', '}', candidate)
                parsed_json = json.loads(candidate)
                break
            except json.JSONDecodeError:
                continue
        
        if parsed_json is None:
            raise ValueError("Unparseable JSON format")
    
    if not isinstance(parsed_json, list):
        parsed_json = [parsed_json] if parsed_json is not None else []
    
    unified = []
    for idx, item in enumerate(parsed_json):
        if not isinstance(item, dict):
            item = {"label": str(item)}

        item_id = str(item["id"])
        
        try:
            label = int(item.get("label"))
            unified.append({"id": item_id, "label": label})
        except (ValueError, TypeError):
            # æ— æ•ˆæ ‡ç­¾ï¼Œå°è¯•ä»æ¦‚ç‡æ¨æ–­
            if prob_mapping and item_id in prob_mapping:
                label_probs = prob_mapping[item_id]
                # æ‰¾åˆ°æ¦‚ç‡æœ€é«˜çš„æ ‡ç­¾
                best_label = None
                best_prob = -1
                
                for label_str, prob in label_probs.items():
                    if prob > best_prob:
                        best_prob = prob
                        best_label = label_str
                
                if best_label is not None:
                    try:
                        inferred_label = int(best_label)
                        unified.append({"id": item_id, "label": inferred_label})
                        print(f"ğŸ”„ æ¨æ–­æ ‡ç­¾: item_id={item_id}, åŸæ ‡ç­¾='{item.get('label')}' â†’ {inferred_label} (æ¦‚ç‡={best_prob:.3f})")
                        continue
                    except (ValueError, TypeError):
                        pass
            
            # å¦‚æœæ¦‚ç‡æ¨æ–­ä¹Ÿå¤±è´¥ï¼Œä½¿ç”¨ä¼ å…¥çš„é»˜è®¤æ ‡ç­¾è€Œä¸æ˜¯è·³è¿‡
            print(f"WARNING:  æ— æ•ˆæ ‡ç­¾ä½¿ç”¨é»˜è®¤å€¼: item_id={item_id}, åŸæ ‡ç­¾='{item.get('label')}' â†’ {default_label} (æ— æ³•æ¨æ–­)")
            unified.append({"id": item_id, "label": default_label})
    
    return unified


def extract_model_prefix(model_name):
    """
    æå–æ¨¡å‹åç§°å‰ç¼€ç”¨äºæ–‡ä»¶å‘½å
    
    è§„åˆ™ï¼š
    1. å¦‚æœæ˜¯ openai:: æ ¼å¼ï¼Œå–åé¢çš„éƒ¨åˆ†
    2. å¦‚æœåŒ…å« /ï¼Œåªå–æœ€åä¸€ä¸ªéƒ¨åˆ†ï¼ˆå¦‚ minzl/toy_3550 -> toy_3550ï¼‰
    3. å…¶ä»–æƒ…å†µç›´æ¥ä½¿ç”¨
    
    Args:
        model_name: å®Œæ•´çš„æ¨¡å‹åç§°
        
    Returns:
        str: ç”¨äºæ–‡ä»¶å‘½åçš„æ¨¡å‹å‰ç¼€
    """
    if '::' in model_name:
        # å¤„ç† backend::model æ ¼å¼
        backend, actual_model = model_name.split('::', 1)
        if backend.lower() == 'openai':
            # å¯¹äº openai::modelï¼Œä½¿ç”¨åé¢çš„æ¨¡å‹å
            model_name = actual_model
        else:
            # å¯¹äºå…¶ä»– backendï¼Œä½¿ç”¨å®Œæ•´çš„æ ¼å¼
            model_name = model_name.replace('::', '_')
    
    # å¤„ç† HuggingFace æ ¼å¼çš„è·¯å¾„ï¼ˆå¦‚ minzl/toy_3550ï¼‰
    if '/' in model_name:
        model_name = model_name.split('/')[-1]
    
    # æ›¿æ¢å…¶ä»–å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦ä¸ºä¸‹åˆ’çº¿
    model_name = model_name.replace('-', '_').replace('.', '_')
    
    return model_name


def construct_filename(dataset_name, seed, train_chunk_size, test_chunk_size, max_samples, temperature, model_name=None):
    """
    æ„å»ºé¢„æµ‹æ–‡ä»¶å
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        seed: éšæœºç§å­
        train_chunk_size: è®­ç»ƒå—å¤§å°
        test_chunk_size: æµ‹è¯•å—å¤§å°
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå·²å¿½ç•¥ï¼‰
        temperature: æ¸©åº¦å‚æ•°ï¼ˆå…¼å®¹æ€§å‚æ•°ï¼Œå·²å¿½ç•¥ï¼‰
        model_name: æ¨¡å‹åç§°ï¼ˆå¯é€‰ï¼Œç”¨äºæ–°æ ¼å¼ï¼‰
        
    Returns:
        str: æ„å»ºçš„æ–‡ä»¶å
    """
    if model_name:
        # æ–°æ ¼å¼ï¼šmodel_prefix@@dataset_Rseed_trainSize_testSize.jsonl
        model_prefix = extract_model_prefix(model_name)
        return f"{model_prefix}@@{dataset_name}_Rseed{seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}.jsonl"
    else:
        # æ—§æ ¼å¼ï¼šdataset_Rseed_trainSize_testSize_pred.jsonl
        return f"{dataset_name}_Rseed{seed}_trainSize{train_chunk_size}_testSize{test_chunk_size}_pred.jsonl"


def perform_weighted_majority_voting(all_votes, all_probabilities, weighted, valid_vote_counts=None):
    """
    æ‰§è¡ŒåŠ æƒå¤šæ•°æŠ•ç¥¨
    
    Args:
        all_votes: æ‰€æœ‰æŠ•ç¥¨æ•°æ®ï¼Œæ ¼å¼ä¸º {key: [(label, weight), ...]}
                  åªåŒ…å«æœ‰æ•ˆæŠ•ç¥¨ï¼ˆtagä¸ºtrueçš„æŠ•ç¥¨ï¼‰
        all_probabilities: æ‰€æœ‰æ¦‚ç‡æ•°æ®ï¼Œç”¨äºAUCè®¡ç®—
        weighted: æ˜¯å¦ä½¿ç”¨æ¦‚ç‡åŠ æƒ
        valid_vote_counts: æ¯ä¸ªkeyçš„æœ‰æ•ˆæŠ•ç¥¨æ•°ï¼Œæ ¼å¼ä¸º {key: count}
        
    Returns:
        tuple: (æŠ•ç¥¨ç»“æœ, æœ€ç»ˆæ¦‚ç‡)
    """
    result = {}
    final_probabilities = {}
    
    for key, votes_with_weights in all_votes.items():
        if not votes_with_weights:
            # æ²¡æœ‰æœ‰æ•ˆæŠ•ç¥¨ï¼Œè·³è¿‡è¿™ä¸ªkey
            continue

        # è·å–è¯¥keyçš„æœ‰æ•ˆæŠ•ç¥¨æ•°
        valid_count = valid_vote_counts.get(key, len(votes_with_weights)) if valid_vote_counts else len(votes_with_weights)
        
        if weighted:
            # Probability-weighted voting: sum probabilities for each label, then normalize by valid vote count
            label_weights = defaultdict(float)
            for label, weight in votes_with_weights:
                label_weights[label] += weight
            
            # å¯é€‰ï¼šæŒ‰æœ‰æ•ˆæŠ•ç¥¨æ•°è¿›è¡Œå½’ä¸€åŒ–ï¼ˆå–å†³äºå…·ä½“éœ€æ±‚ï¼‰
            # è¿™é‡Œä¿æŒç´¯åŠ é€»è¾‘ï¼Œå› ä¸ºæ¦‚ç‡åŠ æƒæœ¬èº«å°±è€ƒè™‘äº†æƒé‡
            
        else:
            # Equal-weight voting: count votes for each label, consider valid vote count
            label_weights = defaultdict(int)
            for label, weight in votes_with_weights:
                label_weights[label] += 1
            
            # å¯¹äºç­‰æƒæŠ•ç¥¨ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—ç™¾åˆ†æ¯”
            # ä½†è¿™é‡Œä»ç„¶ä½¿ç”¨ç»å¯¹è®¡æ•°æ¥å†³å®šè·èƒœæ ‡ç­¾

        if label_weights:
            # Get predicted label (highest total weight/count)
            predicted_label = max(label_weights.items(), key=lambda x: x[1])[0]
            result[key] = predicted_label
            
            # è¾“å‡ºæŠ•ç¥¨è¯¦æƒ…ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            total_weight = sum(label_weights.values())
            winning_weight = label_weights[predicted_label]
            confidence_ratio = winning_weight / total_weight if total_weight > 0 else 0
            
            # åªå¯¹å°‘æ•°keyè¾“å‡ºè¯¦ç»†ä¿¡æ¯ï¼Œé¿å…è¿‡å¤šæ—¥å¿—
            if len(result) <= 3:  # åªæ˜¾ç¤ºå‰3ä¸ªkeyçš„è¯¦æƒ…
                sorted_labels = sorted(label_weights.items(), key=lambda x: x[1], reverse=True)
                print(f"VOTE:  Key {key}: æœ‰æ•ˆæŠ•ç¥¨æ•°={valid_count}, æŠ•ç¥¨åˆ†å¸ƒ={dict(sorted_labels)}, è·èƒœæ ‡ç­¾={predicted_label} (ç½®ä¿¡åº¦={confidence_ratio:.3f})")
            
            # Calculate average probability for positive class
            if key in all_probabilities:
                prob_votes = all_probabilities[key]
                if prob_votes:
                    final_probabilities[key] = sum(prob_votes) / len(prob_votes)
                else:
                    final_probabilities[key] = 0.5
            else:
                final_probabilities[key] = 0.5

    return result, final_probabilities


def parse_classification_report_to_json(report, base_metadata, auc_score):
    """
    é€šç”¨çš„åˆ†ç±»æŠ¥å‘Šè§£æå‡½æ•°ï¼Œé€‚ç”¨äºå•æ–‡ä»¶å’Œæ‰¹é‡è¯„ä¼°
    
    Args:
        report: sklearnåˆ†ç±»æŠ¥å‘Šå­—ç¬¦ä¸²
        base_metadata: åŸºç¡€å…ƒæ•°æ®å­—å…¸
        auc_score: AUCåˆ†æ•°
        
    Returns:
        dict: è§£æåçš„JSONæ•°æ®
    """
    # å¤åˆ¶åŸºç¡€å…ƒæ•°æ®
    json_data = base_metadata.copy()
    json_data["AUC Score"] = auc_score if auc_score is not None else "Not calculated (requires binary classification)"
    
    # è§£æåˆ†ç±»æŠ¥å‘Š
    lines = report.strip().split('\n')
    
    # æå–æ¯ä¸ªç±»åˆ«çš„æŒ‡æ ‡
    precision_values = []
    recall_values = []
    f1_values = []
    support_values = []
    
    # æŸ¥æ‰¾è¡¨æ ¼æ•°æ®è¡Œ
    for line in lines:
        line = line.strip()
        if line.startswith('Class '):
            # è§£æç±»åˆ«è¡Œï¼Œä¾‹å¦‚: "Class 0     0.5000    0.5763    0.5354        59"
            parts = line.split()
            if len(parts) >= 5:
                precision_values.append(float(parts[2]))
                recall_values.append(float(parts[3]))
                f1_values.append(float(parts[4]))
                support_values.append(int(parts[5]))
    
    # æ·»åŠ è¯¦ç»†æŒ‡æ ‡ - æŒ‰ç±»åˆ«åˆ†ç»„
    class_metrics = {}
    for i in range(len(precision_values)):
        class_metrics[f"class_{i}"] = {
            "precision": precision_values[i],
            "recall": recall_values[i],
            "f1-score": f1_values[i],
            "support": support_values[i]
        }
    
    json_data["class_metrics"] = class_metrics
    
    # æå– accuracy, macro avg, weighted avg
    for line in lines:
        line = line.strip()
        if line.startswith('accuracy'):
            # ä¾‹å¦‚: "accuracy                         0.4891       640"
            parts = line.split()
            if len(parts) >= 2:
                json_data["accuracy"] = float(parts[1])
        elif line.startswith('macro avg'):
            # ä¾‹å¦‚: "macro avg     0.4820    0.4911    0.4831       640"
            parts = line.split()
            if len(parts) >= 5:
                json_data["macro_avg_precision"] = float(parts[2])
                json_data["macro_avg_recall"] = float(parts[3])
                json_data["macro_avg_f1"] = float(parts[4])
                json_data["macro_avg_support"] = int(parts[5])
        elif line.startswith('weighted avg'):
            # ä¾‹å¦‚: "weighted avg     0.4862    0.4891    0.4839       640"
            parts = line.split()
            if len(parts) >= 5:
                json_data["weighted_avg_precision"] = float(parts[2])
                json_data["weighted_avg_recall"] = float(parts[3])
                json_data["weighted_avg_f1"] = float(parts[4])
                json_data["weighted_avg_support"] = int(parts[5])
    
    return json_data


def _parse_classification_report(report, dataset_name, model_name, voting_method, 
                               row_shuffle_seeds, 
                               total_combinations, processed_combinations,
                               train_chunk_size, test_chunk_size, 
                               bad_sample_count, auc_score):
    """
    è§£æåˆ†ç±»æŠ¥å‘Šå¹¶ç”Ÿæˆ JSON æ•°æ® (æ‰¹é‡æŠ•ç¥¨ç‰ˆæœ¬)
    """
    # æ„å»ºæ‰¹é‡æŠ•ç¥¨çš„åŸºç¡€å…ƒæ•°æ®
    base_metadata = {
        "Dataset": dataset_name,
        "Model": model_name,
        "Voting Method": voting_method,
        "Default Label Strategy": "Training Data Most Frequent",
        "Random Seeds": row_shuffle_seeds,
        "Total combinations processed": total_combinations,
        "Processed combinations (seeds)": processed_combinations,
        "Chunk sizes": f"train={train_chunk_size}, test={test_chunk_size}",
        "Responses with wrong sample size": bad_sample_count
    }
    
    # ä½¿ç”¨é€šç”¨è§£æå‡½æ•°
    return parse_classification_report_to_json(report, base_metadata, auc_score)


def save_single_file_results(output_txt_file, input_jsonl_file, dataset_name, 
                           report, auc_score, total_samples, bad_sample_count):
    """
    ä¿å­˜å•æ–‡ä»¶è¯„ä¼°ç»“æœ (TXT + JSON)
    
    Args:
        output_txt_file: è¾“å‡ºçš„TXTæ–‡ä»¶è·¯å¾„
        input_jsonl_file: è¾“å…¥çš„JSONLæ–‡ä»¶è·¯å¾„
        dataset_name: æ•°æ®é›†åç§°
        report: åˆ†ç±»æŠ¥å‘Šå­—ç¬¦ä¸²
        auc_score: AUCåˆ†æ•°
        total_samples: æ€»æ ·æœ¬æ•°
        bad_sample_count: é”™è¯¯æ ·æœ¬æ•°
    """
    # æ„å»ºå•æ–‡ä»¶è¯„ä¼°çš„åŸºç¡€å…ƒæ•°æ®
    base_metadata = {
        "Dataset": dataset_name,
        "Input file": input_jsonl_file,
        "Total samples": total_samples,
        "Responses with wrong sample size": bad_sample_count,
        "Evaluation mode": "Direct single file assessment"
    }
    
    # ä½¿ç”¨é€šç”¨è§£æå‡½æ•°ç”ŸæˆJSONæ•°æ®
    json_data = parse_classification_report_to_json(report, base_metadata, auc_score)
    
    # ä¿å­˜TXTæ–‡ä»¶
    with open(output_txt_file, 'w', encoding='utf-8') as f:
        f.write(f"Single File Evaluation Report:\n\n")
        f.write(f"Input file: {input_jsonl_file}\n")
        f.write(f"Total samples processed: {total_samples}\n")
        f.write(f"Responses with wrong sample size: {bad_sample_count}\n\n")
        f.write(report)
        if auc_score is not None:
            f.write(f"\nAUC Score: {auc_score:.4f}\n")
    
    # æ™ºèƒ½ç”ŸæˆJSONæ–‡ä»¶è·¯å¾„
    if output_txt_file.endswith('.txt'):
        json_path = output_txt_file.replace('.txt', '.json')
    elif output_txt_file.endswith('.json'):
        # å¦‚æœç”¨æˆ·æŒ‡å®šçš„æ˜¯.jsonæ–‡ä»¶ï¼Œæˆ‘ä»¬éœ€è¦ç”Ÿæˆå¯¹åº”çš„.txtæ–‡ä»¶
        json_path = output_txt_file
        txt_path = output_txt_file.replace('.json', '.txt')
        # é‡æ–°ä¿å­˜TXTæ–‡ä»¶åˆ°æ­£ç¡®çš„è·¯å¾„
        with open(txt_path, 'w', encoding='utf-8') as f:
            f.write(f"Single File Evaluation Report:\n\n")
            f.write(f"Input file: {input_jsonl_file}\n")
            f.write(f"Total samples processed: {total_samples}\n")
            f.write(f"Responses with wrong sample size: {bad_sample_count}\n\n")
            f.write(report)
            if auc_score is not None:
                f.write(f"\nAUC Score: {auc_score:.4f}\n")
        print(f"SUCCESS: Results saved to: {txt_path}")
    else:
        # å¦‚æœæ²¡æœ‰æ‰©å±•åï¼Œé»˜è®¤æ·»åŠ .json
        json_path = output_txt_file + '.json'
    
    # ä¿å­˜JSONæ–‡ä»¶
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)
        
    if not output_txt_file.endswith('.json'):
        print(f"SUCCESS: Results saved to: {output_txt_file}")
    print(f"SUCCESS: JSON data saved to: {json_path}")


def save_results(result_output_dir, dataset_name, model_name, row_shuffle_seeds,
                train_chunk_size, test_chunk_size, weighted,
                report, auc_score, processed_combinations, bad_sample_count):
    """
    ä¿å­˜è¯„ä¼°ç»“æœ
    """
    seeds_str = '_'.join(map(str, row_shuffle_seeds))
    config_str = f"seeds{seeds_str}"
    
    if weighted:
        voting_type = "probability_weighted_vote"
        voting_method = "Probability-Weighted Majority Voting"
    else:
        voting_type = "equal_weight_vote"
        voting_method = "Equal-Weight Majority Voting"
        
    filename = f"{dataset_name}_{voting_type}_trainSize{train_chunk_size}_{config_str}.txt"
    result_file = os.path.join(result_output_dir, filename)

    with open(result_file, "w", encoding="utf-8") as f:
        f.write(f"Dataset: {dataset_name}\n")
        f.write(f"Model: {model_name}\n")
        f.write(f"Voting Method: {voting_method}\n")
        f.write(f"Default Label Strategy: Training Data Most Frequent\n")
        f.write(f"Random Seeds: {row_shuffle_seeds}\n")
        f.write(f"Total combinations processed: {len(processed_combinations)}\n")
        f.write(f"Processed combinations (seeds): {processed_combinations}\n")
        f.write(f"Chunk sizes: train={train_chunk_size}, test={test_chunk_size}\n")
        f.write(f"Responses with wrong sample size: {bad_sample_count}\n")
        
        if auc_score is not None:
            f.write(f"AUC Score: {auc_score:.4f}\n")
        else:
            f.write("AUC Score: Not calculated (requires binary classification)\n")
            
        f.write("-" * 70 + "\n\n")
        f.write(report)

    # è§£æåˆ†ç±»æŠ¥å‘Šå¹¶ç”Ÿæˆ JSON æ–‡ä»¶
    base_filename = f"{dataset_name}_{voting_type}_trainSize{train_chunk_size}_{config_str}"
    json_file = os.path.join(result_output_dir, f"{base_filename}.json")
    
    json_data = _parse_classification_report(report, dataset_name, model_name, voting_method, 
                                            row_shuffle_seeds, 
                                            len(processed_combinations), processed_combinations,
                                            train_chunk_size, test_chunk_size, 
                                            bad_sample_count, auc_score)
    
    with open(json_file, "w", encoding="utf-8") as f:
        json.dump(json_data, f, indent=2, ensure_ascii=False)

    print(f"SUCCESS: Results saved to: {result_file}")
    print(f"SUCCESS: JSON data saved to: {json_file}")
