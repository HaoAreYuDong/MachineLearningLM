#!/usr/bin/env python3
"""
Layer 5: è¾“å‡ºå±‚
"""

import os
import json
from typing import Dict, Any


class OutputLayer:
    """è¾“å‡ºå±‚ - è´Ÿè´£æ ¼å¼åŒ–å’Œè¾“å‡ºæœ€ç»ˆç»“æœ"""
    
    @staticmethod
    def format_and_output(result: Dict[str, Any], output_dir: str, dataset_name: str, 
                         model_name: str, row_shuffle_seeds: str) -> Dict[str, str]:
        """
        æ ¼å¼åŒ–å¹¶è¾“å‡ºæœ€ç»ˆç»“æœ
        
        Args:
            result: èšåˆåçš„è¯„ä¼°ç»“æœ
            output_dir: è¾“å‡ºç›®å½•
            dataset_name: æ•°æ®é›†åç§°
            model_name: æ¨¡å‹åç§°
            row_shuffle_seeds: éšæœºç§å­
            
        Returns:
            Dict[str, str]: è¾“å‡ºæ–‡ä»¶è·¯å¾„ä¿¡æ¯
        """
        # å¤„ç†æ¨¡å‹åç§°ï¼šåªå–è·¯å¾„çš„æœ€åä¸€éƒ¨åˆ†ï¼Œå¹¶å¤„ç†ç‰¹æ®Šå­—ç¬¦
        model_name_clean = model_name or 'auto_detected'
        
        # å¤„ç† backend::model æ ¼å¼
        if '::' in model_name_clean:
            parts = model_name_clean.split('::', 1)
            backend, actual_model = parts[0], parts[1]
            if backend.lower() == 'openai':
                # å¯¹äº openai::modelï¼Œä½¿ç”¨åé¢çš„æ¨¡å‹å
                model_name_clean = actual_model
            else:
                # å¯¹äºå…¶ä»– backendï¼Œä½¿ç”¨å®Œæ•´çš„æ ¼å¼
                model_name_clean = model_name_clean.replace('::', '_')
        
        # å¤„ç† HuggingFace æ ¼å¼çš„è·¯å¾„ï¼ˆå¦‚ minzl/toy_3550ï¼‰
        if '/' in model_name_clean:
            model_name_clean = model_name_clean.split('/')[-1]
        
        # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦ä¸º @ ç¬¦å·
        safe_model_name = model_name_clean.replace('-', '_').replace('.', '_').replace(':', '@').replace('/', '_')
        
        # å¤„ç† row_shuffle_seedsï¼Œç§»é™¤æ–¹æ‹¬å·å’Œç©ºæ ¼ï¼Œç”¨ @ æ›¿æ¢ç‰¹æ®Šå­—ç¬¦
        safe_seeds = str(row_shuffle_seeds).replace('[', '@').replace(']', '@').replace(' ', '').replace(',', '_').replace(':', '@')
        
        # æ„å»ºå®Œæ•´è¾“å‡ºç›®å½•è·¯å¾„ - ä¿®æ”¹ä¸º dataset_name/model_name çš„å±‚çº§ç»“æ„
        model_output_dir = os.path.join(output_dir, dataset_name, safe_model_name)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(model_output_dir, exist_ok=True)
        
        # ä»ç»“æœä¸­æå– train_chunk_size å’Œ test_chunk_sizeï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        train_size = result.get('train_chunk_size', 'unknown')
        test_size = result.get('test_chunk_size', 'unknown')
        split_seed = result.get('split_seed', 'unknown')
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å - æ–°æ ¼å¼: model_name@@dataset_Sseed*_trainsize*_testsize*_seed@*_report.json/txt
        base_filename = f"{safe_model_name}@@{dataset_name}_Sseed{split_seed}_trainsize{train_size}_testsize{test_size}_seed{safe_seeds}_report"
        json_output_path = os.path.join(model_output_dir, f"{base_filename}.json")
        txt_output_path = os.path.join(model_output_dir, f"{base_filename}.txt")
        
        # æ ¼å¼åŒ–ç»“æœ
        formatted_result = OutputLayer._format_result(result, dataset_name, model_name, row_shuffle_seeds)
        
        # å†™å…¥ JSON æ–‡ä»¶
        with open(json_output_path, 'w', encoding='utf-8') as f:
            json.dump(formatted_result, f, indent=2, ensure_ascii=False)
        
        # å†™å…¥ TXT æ–‡ä»¶
        with open(txt_output_path, 'w', encoding='utf-8') as f:
            f.write(OutputLayer._format_txt_report(formatted_result))
        
        # æ‰“å°æ€»ç»“
        OutputLayer._print_summary(formatted_result)
        
        print(f"\nSAVE: è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"   ğŸ“„ JSON: {json_output_path}")
        print(f"   ğŸ“„ TXT:  {txt_output_path}")
        
        return {
            'json_file': json_output_path,
            'txt_file': txt_output_path,
            'dataset_name': dataset_name,
            'model_name': model_name,
            'seeds': row_shuffle_seeds
        }
    
    @staticmethod
    def _format_result(result: Dict[str, Any], dataset_name: str, model_name: str, seeds: str) -> Dict[str, Any]:
        """æ ¼å¼åŒ–ç»“æœ - å‚è€ƒ b41.json æ ¼å¼"""
        # è·å–æºæ–‡ä»¶ä¿¡æ¯
        source_files = result.get('source_files', [])
        input_file = source_files[0] if source_files else "Unknown"
        
        # å¤„ç†åˆ†ç±»æŠ¥å‘Š
        classification_report_str = result.get('classification_report', 'N/A')
        class_metrics = {}
        macro_avg_precision = None
        macro_avg_recall = None
        macro_avg_f1 = None
        macro_avg_support = None
        weighted_avg_precision = None
        weighted_avg_recall = None
        weighted_avg_f1 = None
        weighted_avg_support = None
        
        # è§£æåˆ†ç±»æŠ¥å‘Šå­—ç¬¦ä¸²ï¼Œæå–å„ç±»åˆ«æŒ‡æ ‡
        if classification_report_str and classification_report_str != 'N/A':
            try:
                lines = classification_report_str.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if line.startswith('Class '):
                        # è§£æç±»åˆ«è¡Œï¼šå¦‚ "Class 0     0.8903    0.9512    0.9198      7978"
                        parts = line.split()
                        if len(parts) >= 5:
                            class_name = f"class_{parts[1]}"
                            precision = float(parts[2])
                            recall = float(parts[3])
                            f1_score = float(parts[4])
                            support = int(parts[5])
                            
                            class_metrics[class_name] = {
                                "precision": precision,
                                "recall": recall,
                                "f1-score": f1_score,
                                "support": support
                            }
                    elif line.startswith('macro avg'):
                        # è§£æ macro avg è¡Œ
                        parts = line.split()
                        if len(parts) >= 5:
                            macro_avg_precision = float(parts[2])
                            macro_avg_recall = float(parts[3])
                            macro_avg_f1 = float(parts[4])
                            macro_avg_support = int(parts[5])
                    elif line.startswith('weighted avg'):
                        # è§£æ weighted avg è¡Œ
                        parts = line.split()
                        if len(parts) >= 5:
                            weighted_avg_precision = float(parts[2])
                            weighted_avg_recall = float(parts[3])
                            weighted_avg_f1 = float(parts[4])
                            weighted_avg_support = int(parts[5])
            except Exception as e:
                print(f"WARNING: è§£æåˆ†ç±»æŠ¥å‘Šæ—¶å‡ºé”™: {e}")
        
        # å¤„ç† AUC Score
        auc_score = result.get('auc_score', None)
        auc_message = auc_score if auc_score is not None else "Not calculated (requires binary classification)"
        
        # ç”Ÿæˆç±»ä¼¼ b41.json çš„æ ¼å¼
        formatted = {
            "Dataset": dataset_name,
            "Input file": input_file,
            "Total samples": result.get('total_samples', 0),
            "Responses with wrong sample size": 0,  # å‡è®¾é»˜è®¤ä¸º0ï¼Œå¯ä»¥åç»­æ·»åŠ æ­¤ç»Ÿè®¡
            "Evaluation mode": result.get('aggregation_mode', 'unknown').replace('_', ' ').title(),
            "AUC Score": auc_message,
            "class_metrics": class_metrics
        }
        
        # æ·»åŠ  macro å’Œ weighted å¹³å‡å€¼ï¼ˆå¦‚æœè§£ææˆåŠŸï¼‰
        if macro_avg_precision is not None:
            formatted["macro_avg_precision"] = macro_avg_precision
            formatted["macro_avg_recall"] = macro_avg_recall
            formatted["macro_avg_f1"] = macro_avg_f1
            formatted["macro_avg_support"] = macro_avg_support
            
        if weighted_avg_precision is not None:
            formatted["weighted_avg_precision"] = weighted_avg_precision
            formatted["weighted_avg_recall"] = weighted_avg_recall
            formatted["weighted_avg_f1"] = weighted_avg_f1
            formatted["weighted_avg_support"] = weighted_avg_support
        
        return formatted
    
    @staticmethod
    def _print_summary(result: Dict[str, Any]):
        """æ‰“å°è¯„ä¼°æ€»ç»“ - ä¸TXTæ–‡ä»¶æ ¼å¼ä¿æŒä¸€è‡´"""
        print("\n" + "-" * 70)
        print("EVALUATION SUMMARY")
        print("-" * 70)
        
        # åŸºæœ¬ä¿¡æ¯ - ç®€æ´æ ¼å¼ï¼Œä¸TXTä¸€è‡´
        print(f"Dataset: {result.get('Dataset', 'N/A')}")
        
        # ä»è¾“å…¥æ–‡ä»¶è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
        input_file = result.get('Input file', '')
        if '@@' in input_file:
            model_part = input_file.split('@@')[0].split('/')[-1] if input_file else 'N/A'
        else:
            model_part = 'N/A'
        print(f"Model: {model_part}")
        
        print(f"Evaluation Mode: {result.get('Evaluation mode', 'N/A')}")
        print(f"Total samples: {result.get('Total samples', 0)}")
        print(f"Responses with wrong sample size: {result.get('Responses with wrong sample size', 0)}")
        
        # AUC Score
        auc_score = result.get('AUC Score')
        if auc_score is not None and isinstance(auc_score, (int, float)):
            print(f"AUC Score: {auc_score:.4f}")
        else:
            print(f"AUC Score: {auc_score}")
        
        print("-" * 70)
        print()
        
        # é‡æ–°æ„å»ºåˆ†ç±»æŠ¥å‘Šçš„åŸå§‹æ ¼å¼ - ä¸TXTå®Œå…¨ä¸€è‡´
        class_metrics = result.get('class_metrics', {})
        if class_metrics:
            # è¡¨å¤´
            print("              precision    recall  f1-score   support")
            print()
            
            # å„ç±»åˆ«è¡Œ
            for class_name in sorted(class_metrics.keys()):
                metrics = class_metrics[class_name]
                class_num = class_name.replace('class_', '')
                precision = metrics.get('precision', 0)
                recall = metrics.get('recall', 0)
                f1_score = metrics.get('f1-score', 0)
                support = metrics.get('support', 0)
                
                print(f"     Class {class_num}     {precision:.4f}    {recall:.4f}    {f1_score:.4f}      {support}")
            
            print()
            
            # è®¡ç®— accuracy
            total_correct = 0
            total_samples = 0
            for metrics in class_metrics.values():
                # accuracy è®¡ç®—éœ€è¦æ‰€æœ‰ç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°
                recall = metrics.get('recall', 0)
                support = metrics.get('support', 0)
                total_correct += recall * support
                total_samples += support
            
            accuracy = total_correct / total_samples if total_samples > 0 else 0
            print(f"    accuracy                         {accuracy:.4f}      {total_samples}")
            
            # macro avg å’Œ weighted avg
            if result.get('macro_avg_precision') is not None:
                print(f"   macro avg     {result.get('macro_avg_precision', 0):.4f}    {result.get('macro_avg_recall', 0):.4f}    {result.get('macro_avg_f1', 0):.4f}      {result.get('macro_avg_support', 0)}")
            
            if result.get('weighted_avg_precision') is not None:
                print(f"weighted avg     {result.get('weighted_avg_precision', 0):.4f}    {result.get('weighted_avg_recall', 0):.4f}    {result.get('weighted_avg_f1', 0):.4f}      {result.get('weighted_avg_support', 0)}")
        
        print()
        print("-" * 70)

    @staticmethod
    def _format_txt_report(result: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æŠ¥å‘Š - æ¢å¤åŸå§‹ç®€æ´æ ¼å¼ï¼Œæ–¹ä¾¿äººç±»é˜…è¯»"""
        lines = []
        
        # åŸºæœ¬ä¿¡æ¯ - ç®€æ´æ ¼å¼
        lines.append(f"Dataset: {result.get('Dataset', 'N/A')}")
        
        # ä»è¾“å…¥æ–‡ä»¶è·¯å¾„ä¸­æå–æ¨¡å‹åç§°
        input_file = result.get('Input file', '')
        if '@@' in input_file:
            model_part = input_file.split('@@')[0].split('/')[-1] if input_file else 'N/A'
        else:
            model_part = 'N/A'
        lines.append(f"Model: {model_part}")
        
        lines.append(f"Evaluation Mode: {result.get('Evaluation mode', 'N/A')}")
        lines.append(f"Total samples: {result.get('Total samples', 0)}")
        lines.append(f"Responses with wrong sample size: {result.get('Responses with wrong sample size', 0)}")
        
        # AUC Score
        auc_score = result.get('AUC Score')
        if auc_score is not None and isinstance(auc_score, (int, float)):
            lines.append(f"AUC Score: {auc_score:.4f}")
        else:
            lines.append(f"AUC Score: {auc_score}")
        
        lines.append("-" * 70)
        lines.append("")
        
        # é‡æ–°æ„å»ºåˆ†ç±»æŠ¥å‘Šçš„åŸå§‹æ ¼å¼
        class_metrics = result.get('class_metrics', {})
        if class_metrics:
            # è¡¨å¤´
            lines.append("              precision    recall  f1-score   support")
            lines.append("")
            
            # å„ç±»åˆ«è¡Œ
            for class_name in sorted(class_metrics.keys()):
                metrics = class_metrics[class_name]
                class_num = class_name.replace('class_', '')
                precision = metrics.get('precision', 0)
                recall = metrics.get('recall', 0)
                f1_score = metrics.get('f1-score', 0)
                support = metrics.get('support', 0)
                
                lines.append(f"     Class {class_num}     {precision:.4f}    {recall:.4f}    {f1_score:.4f}      {support}")
            
            lines.append("")
            
            # è®¡ç®— accuracy
            total_correct = 0
            total_samples = 0
            for metrics in class_metrics.values():
                # accuracy è®¡ç®—éœ€è¦æ‰€æœ‰ç±»åˆ«çš„æ­£ç¡®é¢„æµ‹æ•°
                recall = metrics.get('recall', 0)
                support = metrics.get('support', 0)
                total_correct += recall * support
                total_samples += support
            
            accuracy = total_correct / total_samples if total_samples > 0 else 0
            lines.append(f"    accuracy                         {accuracy:.4f}      {total_samples}")
            
            # macro avg å’Œ weighted avg
            if result.get('macro_avg_precision') is not None:
                lines.append(f"   macro avg     {result.get('macro_avg_precision', 0):.4f}    {result.get('macro_avg_recall', 0):.4f}    {result.get('macro_avg_f1', 0):.4f}      {result.get('macro_avg_support', 0)}")
            
            if result.get('weighted_avg_precision') is not None:
                lines.append(f"weighted avg     {result.get('weighted_avg_precision', 0):.4f}    {result.get('weighted_avg_recall', 0):.4f}    {result.get('weighted_avg_f1', 0):.4f}      {result.get('weighted_avg_support', 0)}")
        
        lines.append("")
        
        return "\n".join(lines)
        """æ ¼å¼åŒ–ä¸ºæ–‡æœ¬æŠ¥å‘Š - é€‚é…æ–°çš„JSONæ ¼å¼"""
        lines = []
        
        # æ ‡é¢˜
        lines.append("="*60)
        lines.append("ğŸ† äº”å±‚æ™ºèƒ½æ¶æ„è¯„ä¼°æŠ¥å‘Š")
        lines.append("="*60)
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append(f"INFO: æ•°æ®é›†: {result.get('Dataset', 'N/A')}")
        lines.append(f"ï¿½ è¾“å…¥æ–‡ä»¶: {result.get('Input file', 'N/A')}")
        lines.append(f"ğŸ“ˆ æ€»æ ·æœ¬æ•°: {result.get('Total samples', 0)}")
        lines.append(f"ï¿½ï¸  è¯„ä¼°æ¨¡å¼: {result.get('Evaluation mode', 'N/A')}")
        
        # AUC Score
        auc_score = result.get('AUC Score')
        if auc_score is not None:
            if isinstance(auc_score, (int, float)):
                lines.append(f"ï¿½ AUC Score: {auc_score:.4f}")
            else:
                lines.append(f"TARGET: AUC Score: {auc_score}")
        
        # ç±»åˆ«æŒ‡æ ‡
        class_metrics = result.get('class_metrics', {})
        if class_metrics:
            lines.append("")
            lines.append("ğŸ“ˆ åˆ†ç±»è¯¦ç»†æŒ‡æ ‡:")
            for class_name, metrics in class_metrics.items():
                class_num = class_name.replace('class_', '')
                lines.append(f"   Class {class_num}:")
                lines.append(f"     Precision: {metrics.get('precision', 0):.4f}")
                lines.append(f"     Recall: {metrics.get('recall', 0):.4f}")
                lines.append(f"     F1-Score: {metrics.get('f1-score', 0):.4f}")
                lines.append(f"     Support: {metrics.get('support', 0)}")
        
        # Macro å¹³å‡
        if result.get('macro_avg_precision') is not None:
            lines.append("")
            lines.append("INFO: Macro å¹³å‡:")
            lines.append(f"   Precision: {result.get('macro_avg_precision', 0):.4f}")
            lines.append(f"   Recall: {result.get('macro_avg_recall', 0):.4f}")
            lines.append(f"   F1-Score: {result.get('macro_avg_f1', 0):.4f}")
            lines.append(f"   Support: {result.get('macro_avg_support', 0)}")
        
        # Weighted å¹³å‡
        if result.get('weighted_avg_precision') is not None:
            lines.append("")
            lines.append("ï¿½ Weighted å¹³å‡:")
            lines.append(f"   Precision: {result.get('weighted_avg_precision', 0):.4f}")
            lines.append(f"   Recall: {result.get('weighted_avg_recall', 0):.4f}")
            lines.append(f"   F1-Score: {result.get('weighted_avg_f1', 0):.4f}")
            lines.append(f"   Support: {result.get('weighted_avg_support', 0)}")
        
        lines.append("")
        lines.append("="*60)
        
        return "\n".join(lines)
