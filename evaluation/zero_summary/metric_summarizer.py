#!/usr/bin/env python3
"""
ç®€åŒ–ç‰ˆ Metric Results Summarizer
"""

import os
import sys
import json
import pandas as pd
import glob
import re
from datetime import datetime
import pytz


def get_beijing_time():
    """è·å–åŒ—äº¬æ—¶é—´æ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼ˆæœˆæ—¥æ—¶åˆ†ï¼‰"""
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now = datetime.now(beijing_tz)
    return now.strftime("%m%d%H%M")


def extract_model_prefix(model_name):
    """æå–æ¨¡å‹åç§°å‰ç¼€"""
    # å¤„ç† backend::model æ ¼å¼
    if '::' in model_name:
        parts = model_name.split('::', 1)
        backend, actual_model = parts[0], parts[1]
        if backend.lower() == 'openai':
            # å¯¹äº openai::modelï¼Œä½¿ç”¨åé¢çš„æ¨¡å‹å
            model_name = actual_model
        else:
            # å¯¹äºå…¶ä»– backendï¼Œä½¿ç”¨å®Œæ•´çš„æ ¼å¼
            model_name = model_name.replace('::', '_')
    
    # å¤„ç† HuggingFace æ ¼å¼çš„è·¯å¾„ï¼ˆå¦‚ minzl/toy_3550ï¼‰
    if '/' in model_name:
        model_name = model_name.split('/')[-1]
    
    # æ›¿æ¢æ‰€æœ‰å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
    return model_name.replace('-', '_').replace('.', '_').replace(':', '@').replace('/', '_')


def parse_json_file(json_file_path):
    """è§£æJSONæ–‡ä»¶"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ä»Input fileè§£æä¿¡æ¯
        input_file = data.get('Input file', '')
        filename = os.path.basename(input_file)
        
        # æå–æ¨¡å‹åç§°
        model_name = 'unknown'
        if '@@' in filename:
            model_name = filename.split('@@')[0]
        
        # æå–è®­ç»ƒå’Œæµ‹è¯•å¤§å° - é€‚é…æ–°çš„æ–‡ä»¶å‘½åæ ¼å¼
        train_size = None
        test_size = None
        # æ–°æ ¼å¼: model@@dataset_Sseed*_trainsize*_testsize*_seed@*_report
        train_match = re.search(r'trainsize(\d+)', input_file)
        test_match = re.search(r'testsize(\d+)', input_file)
        if train_match:
            train_size = int(train_match.group(1))
        if test_match:
            test_size = int(test_match.group(1))
        
        # å¦‚æœæ–°æ ¼å¼æ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•æ—§æ ¼å¼å…¼å®¹
        if train_size is None:
            train_match_old = re.search(r'trainSize(\d+)', input_file)
            if train_match_old:
                train_size = int(train_match_old.group(1))
        
        if test_size is None:
            test_match_old = re.search(r'testSize(\d+)', input_file)
            if test_match_old:
                test_size = int(test_match_old.group(1))
        
        # è¯»å–accuracyï¼ˆä»txtæ–‡ä»¶ï¼‰
        accuracy = None
        txt_file_path = json_file_path.replace('.json', '.txt')
        if os.path.exists(txt_file_path):
            with open(txt_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            accuracy_match = re.search(r'accuracy\s+(\d+\.\d+)', content)
            if accuracy_match:
                accuracy = float(accuracy_match.group(1))
        
        # è·å–ç±»åˆ«æŒ‡æ ‡
        class_metrics = data.get('class_metrics', {})
        f1_c0 = class_metrics.get('class_0', {}).get('f1-score', None)
        f1_c1 = class_metrics.get('class_1', {}).get('f1-score', None)
        
        # å¤„ç†AUC
        auc_score = data.get('AUC Score', None)
        if auc_score is None or auc_score == "Not calculated (requires binary classification)":
            auc_score = -1
        
        return {
            'dataset': data.get('Dataset', 'unknown'),
            'train_size': train_size,
            'test_size': test_size,
            'model_name': model_name,
            'total_samples': data.get('Total samples', 0),
            'error_samples': data.get('Responses with wrong sample size', 0),
            'f1_c0': f1_c0,
            'f1_c1': f1_c1,
            'f1_w': data.get('weighted_avg_f1', None),
            'auc': auc_score,
            'accuracy': accuracy
        }
        
    except Exception as e:
        print(f"Error parsing {json_file_path}: {e}")
        return None


def main():
    
    
    if len(sys.argv) < 5:
        print("Usage: python script.py --metric_data_dir DIR --report_data_dir DIR [--model_name NAME]")
        return 1
    
    # ç®€å•è§£æå‚æ•°
    metric_data_dir = None
    report_data_dir = None
    model_name_hint = None
    
    for i, arg in enumerate(sys.argv):
        if arg == '--metric_data_dir' and i + 1 < len(sys.argv):
            metric_data_dir = sys.argv[i + 1]
        elif arg == '--report_data_dir' and i + 1 < len(sys.argv):
            report_data_dir = sys.argv[i + 1]
        elif arg == '--model_name' and i + 1 < len(sys.argv):
            model_name_hint = sys.argv[i + 1]
    
    if not metric_data_dir or not report_data_dir:
        print("Both --metric_data_dir and --report_data_dir are required")
        return 1
    
    print(f"STARTING: å¼€å§‹ç»Ÿè®¡metricæ•°æ®...")
    print(f"INPUT: è¾“å…¥ç›®å½•: {metric_data_dir}")
    print(f"OUTPUT: è¾“å‡ºç›®å½•: {report_data_dir}")
    if model_name_hint:
        print(f"ğŸ·ï¸  æ¨¡å‹åç§°: {model_name_hint}")
    print()
    
    # æŸ¥æ‰¾JSONæ–‡ä»¶
    json_files = glob.glob(os.path.join(metric_data_dir, "**", "*.json"), recursive=True)
    print(f"OUTPUT: æ‰¾åˆ° {len(json_files)} ä¸ªJSONæ–‡ä»¶")
    
    if not json_files:
        print("ERROR: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶")
        return 1
    
    # è§£ææ‰€æœ‰æ–‡ä»¶
    results = []
    for json_file in json_files:
        result = parse_json_file(json_file)
        if result:
            results.append(result)
    
    print(f"SUCCESS: æˆåŠŸè§£æ {len(results)}/{len(json_files)} ä¸ªæ–‡ä»¶")
    
    if not results:
        print("ERROR: æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ–‡ä»¶")
        return 1
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(results)
    
    # æŒ‰datasetæ’åºï¼Œç„¶åæŒ‰train_sizeæ’åº
    df_sorted = df.sort_values(['dataset', 'train_size'], ascending=[True, True])
    
    # ç¡®å®šæ¨¡å‹å‰ç¼€
    if model_name_hint:
        model_prefix = extract_model_prefix(model_name_hint)
    else:
        # ä½¿ç”¨æœ€å¸¸è§çš„æ¨¡å‹å
        model_names = [r['model_name'] for r in results if r['model_name'] != 'unknown']
        if model_names:
            model_prefix = extract_model_prefix(model_names[0])
        else:
            model_prefix = 'unknown'
    
    # ç”Ÿæˆæ–‡ä»¶å
    beijing_time = get_beijing_time()
    csv_filename = f"{model_prefix}_{beijing_time}_results.csv"
    csv_path = os.path.join(report_data_dir, csv_filename)
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(report_data_dir, exist_ok=True)
    
    # æŒ‰æŒ‡å®šé¡ºåºæ’åˆ—åˆ—
    column_order = [
        'dataset', 'train_size', 'test_size', 'model_name', 'total_samples',
        'error_samples', 'f1_c0', 'f1_c1', 'f1_w', 'auc', 'accuracy'
    ]
    
    df_ordered = df_sorted.reindex(columns=column_order)
    
    # ç¾åŒ–CSVè¾“å‡ºï¼šåœ¨æ•°æ®é›†ä¹‹é—´æ’å…¥ç©ºè¡Œ
    def write_formatted_csv(df, file_path):
        """å†™å…¥æ ¼å¼åŒ–çš„CSVæ–‡ä»¶ï¼Œåœ¨æ•°æ®é›†ä¹‹é—´æ’å…¥ç©ºè¡Œ"""
        with open(file_path, 'w', encoding='utf-8', newline='') as f:
            # å†™å…¥è¡¨å¤´
            f.write(','.join(df.columns) + '\n')
            
            current_dataset = None
            for _, row in df.iterrows():
                # å¦‚æœæ˜¯æ–°çš„æ•°æ®é›†ä¸”ä¸æ˜¯ç¬¬ä¸€è¡Œï¼Œæ’å…¥ç©ºè¡Œ
                if current_dataset is not None and row['dataset'] != current_dataset:
                    f.write('\n')
                
                # å†™å…¥æ•°æ®è¡Œ
                row_values = []
                for col in df.columns:
                    value = row[col]
                    # æ ¼å¼åŒ–æ•°å€¼ï¼Œä¿æŒç²¾åº¦
                    if isinstance(value, float) and not pd.isna(value):
                        if col in ['f1_c0', 'f1_c1', 'f1_w', 'accuracy']:
                            row_values.append(f"{value:.4f}")
                        elif col == 'auc':
                            row_values.append(f"{value:.6f}")
                        else:
                            row_values.append(str(value))
                    else:
                        row_values.append(str(value) if not pd.isna(value) else '')
                
                f.write(','.join(row_values) + '\n')
                current_dataset = row['dataset']
    
    # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°å†™å…¥CSV
    write_formatted_csv(df_ordered, csv_path)
    
    # ç»Ÿè®¡ä¿¡æ¯
    datasets_count = df_ordered['dataset'].nunique()
    train_sizes = sorted(df_ordered['train_size'].dropna().unique())
    
    print(f"INFO: CSVæ–‡ä»¶å·²ç”Ÿæˆ: {csv_path}")
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ•°æ®é›†æ•°é‡: {datasets_count}")
    print(f"   - è®­ç»ƒå¤§å°: {train_sizes}")
    print(f"   - æ€»è®°å½•æ•°: {len(df_ordered)}")
    print(f"   - æ’åºæ–¹å¼: æ•°æ®é›†åç§° â†’ è®­ç»ƒå¤§å°")
    print(f"   - æ ¼å¼åŒ–: æ•°æ®é›†é—´æ’å…¥ç©ºè¡Œ")
    return 0


if __name__ == "__main__":
    exit(main())
