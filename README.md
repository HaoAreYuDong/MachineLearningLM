# Machine Learning LLM Evaluation Framework

A comprehensive framework for evaluating Large Language Models on machine learning tasks, supporting both traditional machine learning models and deep learning approaches with automated pipeline processing.

## Overview

This framework provides end-to-end evaluation capabilities for LLMs on machine learning tasks, featuring automated data preprocessing, prompt generation, model inference, and comprehensive evaluation metrics.

## Important Notes

1. **Special Character Handling**: Due to shell reserved characters, CSV filenames in TALENT datasets may contain special characters like `(` which are shell reserved characters. We recommend preprocessing these filenames to use only numbers, letters, and underscores before processing.

2. **Text Data Processing**: While we support text data processing, since we use commas (`,`) as feature separators, please replace any commas in your dataset text to avoid model confusion. In our evaluation, we use spaces as replacements for commas.

## Batch Processing Usage

For batch processing, you need to provide **input path** and **output path** parameters. The framework supports three execution modes:

### Step 1: Activate Parameters
```bash
source ./scripts/parameters.sh
```

### Step 2: Choose Execution Mode
See the "Execution Options" section below for detailed commands based on your preferred processing mode (sequential, parallel, or end-to-end pipeline).

## Execution Options

### Option 1: Sequential Processing
Use scripts in `single_process/` directory to run steps sequentially:
```bash
./scripts/single_process/data_prep.sh
./scripts/single_process/prompt_gen.sh  # For deep learning only
./scripts/single_process/model_pred.sh
./scripts/single_process/evaluation.sh
./scripts/single_process/report.sh
```

### Option 2: Parallel Processing
Use scripts in `multi_process/` directory for accelerated parallel execution:
```bash
./scripts/multi_process/data_prep.sh
./scripts/multi_process/prompt_gen.sh  # For deep learning only
./scripts/multi_process/model_pred.sh
./scripts/multi_process/evaluation.sh
./scripts/multi_process/report.sh
```

### Option 3: End-to-End Pipeline
Run the complete pipeline with parallelization optimizations:
```bash
./scripts/pipeline.sh
```

## Single File Processing

For direct inference on individual JSONL files, we support single-file processing mode.

**Important**: The input file must have a `.jsonl` extension - the code logic uses this suffix for file type identification.

The file should contain prompts in LLaMA Factory's Alpaca format with the following structure:
- `instruction`: The task instruction
- `input`: The input data
- `output`: The expected output

### Local Model Usage Example
```bash
python ./src/evaluation/model_pred/dl_model_pred.py \
  --input_dir ./demo_input.jsonl \
  --output_dir ./demo_output.jsonl \
  --model_name hf_repo/model_name
```

### Cloud Model Usage
For cloud model calls, model path must start with `openai::` for proper parsing and OpenAI SDK execution:

```bash
python3 ./src/evaluation/model_pred/dl_model_pred.py \
  --input_dir ./input_demo.jsonl \
  --output_dir ./output_demo.jsonl \
  --model_name openai::gpt-4o-mini \
  --api_key your_own_api_key \
  --base_url your_own_base_url \
  --max_samples 5
```

## Single File Evaluation

You can also perform evaluation on individual files directly:

```bash
python ./src/evaluation/result_proc/evaluator.py \
  --input_dir ./demo_response.jsonl \
  --output_dir ./output_demo.txt   # Can also be .jsonl
```

**Note**: Our evaluation framework is specifically designed for results generated by our `dl_model_pred` inference pipeline. Please use outputs from our inference module as input for evaluation to ensure compatibility.

## Tabicl Evaluation


## Project Structure

```
MachineLearningLM/
├──src/
|   ├──evaluation/
│   │   ├── data_prep/          # Data preprocessing and chunking utilities
│   │   ├── prompt_gen/         # Prompt generation for deep learning models
│   │   ├── model_pred/         # Model inference (ML and DL prediction engines)
│   │   ├── result_proc/        # 5-layer evaluation architecture and metrics processing
│   │   ├── zero_summary/       # Result summarization and report generation
│   │   └── tabicl_evaluate.py
│   └──prior_data
│       ├── tabicl/
│       └── pt_to_csv.py       
├── scripts/
│   ├── single_process/     # Sequential execution shell scripts
│   ├── multi_process/      # Parallel execution shell scripts (with _mp suffix)
│   ├── evaluate_parameters.sh       # Global parameter configuration
|   └── evaluate_pipeline.sh         # automated pipeline
├── datahub_inputs/
│   ├── data_demo/          # Demo datasets for testing
│   └── data_raw/           # Raw input datasets
├── requirements.txt        # Python dependencies
└── README.md
```

## Installation

```bash
# Install Python dependencies
pip install -r requirements.txt
```

## Configuration

All parameters are managed through `./scripts/parameters.sh`. Modify this file to customize:
- Input/output paths
- Model configurations
- Processing parameters
- Evaluation settings

## Features

- **Dual Model Support**: Traditional ML and Deep Learning models
- **Flexible Processing**: Single-process or multi-process execution
- **Automated Pipeline**: End-to-end workflow automation
- **Single File Support**: Direct inference on individual JSONL files
- **Comprehensive Evaluation**: Multi-metric evaluation framework
- **Parallel Optimization**: Built-in parallelization for performance
